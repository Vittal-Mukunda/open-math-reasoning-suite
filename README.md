# ğŸ§  AIMO Project â€“ Local Mathematical Olympiad Evaluation System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)

> **A complete environment for loading, solving, submitting, and scoring AI-generated mathematical reasoning tasks.**

---

## ğŸ“– Overview

This repository simulates an **AI Mathematical Olympiad (AIMO)** competition environment.  
It provides a local evaluation sandbox identical to official AI reasoning competitions.

With this framework, you can:

- ğŸ“¥ **Load** Olympiad-style math problems  
- ğŸ§  **Develop & run** custom solvers  
- ğŸ“ **Submit** answers programmatically  
- ğŸ“Š **Score** predictions using competition rules  
- ğŸ“„ **Generate** automatic `submission.csv` submissions  

---

# ğŸ“‚ AIMO Project â€“ Full File-by-File Documentation

This document explains **every folder and every file** inside the AIMO Project repository, based entirely on your provided directory structure.  
Nothing is omitted. This is the **complete and official technical breakdown** suitable for a GitHub README.

---

# ğŸ—ï¸ Repository Structure (Explained in Detail)

Below is the full project layout you uploaded:

```
AIMO_Project/
â”‚
â”œâ”€â”€ aimo_api/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ client.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ ordering.py
â”‚   â”œâ”€â”€ scorer.py
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ simple_baseline.py
â”‚   â”œâ”€â”€ solver.py
â”‚
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ scoring.py
â”‚
â”œâ”€â”€ problems/
â”‚   â”œâ”€â”€ problems.json
â”‚   â”œâ”€â”€ sample_problems.json
â”‚
â”œâ”€â”€ solver/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_solver.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run1_submission.csv
â”œâ”€â”€ submission.csv
â”œâ”€â”€ submission_template.py
â”œâ”€â”€ test_runner.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
```

---

# ğŸ”µ 1. Folder: `aimo_api/`

A legacy or alternative version of the API/evaluator system. It contains an older pipeline for loading problems, ordering them, running scoring, and interacting with a client. Likely preserved for backward compatibility.

## ğŸ“„ `aimo_api/client.py`
- Earlier version of the main API client.
- Handles:
  - Loading problems
  - Serving problems sequentially
  - Accepting solver submissions
  - Tracking state

## ğŸ“„ `aimo_api/loader.py`
- Loads datasets (JSON files) used by the competition.
- Validates entries and returns structured problem lists.

## ğŸ“„ `aimo_api/ordering.py`
- Controls the order in which problems are delivered.
- Can include:
  - Shuffling
  - Sorting
  - Curriculum ordering
  - Seeding for reproducibility

## ğŸ“„ `aimo_api/scorer.py`
- Legacy scoring system.
- Compares predictions to answers.
- Computes score breakdowns.

## ğŸ“ `aimo_api/__pycache__/`
- Auto-generated Python bytecode.
- Not important; ignored by Git.

---

# ğŸ”µ 2. Folder: `api/`

This is the **active, modern API layer** used by solvers.

## ğŸ“„ `api/__init__.py`
- Initializes the API package.
- Enables imports like:

```python
from api import client
```

---

# ğŸŸ¢ 3. Folder: `baselines/`

Contains **baseline solver examples** used for testing and demonstrating the environment.

## ğŸ“„ `baselines/simple_baseline.py`
- A very simple solver for demonstration.
- Responsibilities:
  - Fetch problem using `AIMOClient`
  - Produce trivial or default answers
  - Submit those answers back to evaluator

## ğŸ“„ `baselines/solver.py`
- Base solver class or template.
- Provides reusable logic for other solvers.
- Sometimes contains:
  - Parsing utilities
  - Generic solve() methods
  - A structure to extend custom solvers

## ğŸ“ `baselines/__pycache__/`
- Auto-generated Python cache files.

---

# ğŸŸ  4. Folder: `evaluator/`

This is the **official scoring and data-loading engine** for the entire system.

## ğŸ“„ `evaluator/__init__.py`
- Initializes evaluator package.

## ğŸ“„ `evaluator/loader.py`
- Loads/validates problem JSON files.
- Ensures each problem has:
  - `id`
  - `latex`
  - `answer`
- Checks uniqueness of IDs.

## ğŸ“„ `evaluator/scoring.py`
- Main scoring script.
- Compares solver predictions vs. ground truth.
- Computes:
  - Total solved
  - Correct count
  - Accuracy percentage
- Generates:
  - `submission.csv`
  - Detailed scoring breakdown

---

# ğŸŸ£ 5. Folder: `problems/`

Contains datasets the solver will use.

## ğŸ“„ `problems/problems.json`
- A full or alternative dataset of problems.

## ğŸ“„ `problems/sample_problems.json`
- Default dataset loaded by `test_runner.py`.
- Example structure:

```json
{
  "id": "p1",
  "latex": "Compute 2 + 2.",
  "answer": 4
}
```

---

# ğŸŸ¤ 6. Folder: `solver/`

This contains **your actual solver implementation** (not baseline).

## ğŸ“„ `solver/__init__.py`
- Package initializer.

## ğŸ“„ `solver/model_solver.py`
- Your custom solver.
- Can implement:
  - LLM-based reasoning
  - Rule-based math solving
  - Heuristic systems
- Integrated with `AIMOClient`.

This is the file YOU modify to create an intelligent agent.

---

# âš« 7. Project Root Files

## ğŸ“„ `.gitignore`
Specifies which files Git should ignore, e.g.:
- `__pycache__/`
- `.env`
- `*.pyc`
- `submission.csv`

## ğŸ“„ `README.md`
Your project documentation.

## ğŸ“„ `requirements.txt`
Lists dependencies. Installed via:

```bash
pip install -r requirements.txt
```

## ğŸ“„ `run1_submission.csv`
A **sample submission** produced by a past run.

## ğŸ“„ `submission.csv`
The **latest generated submission**, created by scoring after running:

```bash
python test_runner.py
```

## ğŸ“„ `submission_template.py`
Template file for producing competition-style submissions.
Contains:
- Example solve loop
- Submission formatting rules

## ğŸ“„ `test_runner.py`
ğŸ”¥ The **central orchestrator** for the entire system.

**Responsibilities:**
1. Load problems from `/problems/sample_problems.json`
2. Create an AIMOClient instance
3. Run a solver (baseline or your custom solver)
4. Collect predictions
5. Score using evaluator/scoring.py
6. Save results to `submission.csv`
7. Print scoring summary

Used like:

```bash
python test_runner.py
```

## ğŸ“„ `LICENSE`
License file (MIT).

## ğŸ“„ `README.md` (duplicate)
A second README, probably accidental. Should remove one to avoid confusion.

---

# âœ… Summary

This document represents the **entire technical breakdown of every file** in your repository.

Use it directly as:
âœ” Documentation  
âœ” Contributor onboarding  
âœ” GitHub README section  
âœ” Internal team reference  

---


## â­ Acknowledgements

Inspired by academic AI reasoning benchmarks including:

- AI Mathematical Olympiads  
- MATH dataset  
- GSM8K reasoning tasks  

---
