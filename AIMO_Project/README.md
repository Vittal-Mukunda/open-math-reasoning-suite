# üß† AIMO Project ‚Äì Local Mathematical Olympiad Evaluation System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)

> **A complete environment for loading, solving, submitting, and scoring AI-generated mathematical reasoning tasks.**

---

## üìñ Overview

This repository simulates an **AI Mathematical Olympiad (AIMO)** competition environment.  
It provides a local evaluation sandbox identical to official AI reasoning competitions.

With this framework, you can:

- üì• **Load** Olympiad-style math problems  
- üß† **Develop & run** custom solvers  
- üìù **Submit** answers programmatically  
- üìä **Score** predictions using competition rules  
- üìÑ **Generate** automatic `submission.csv` submissions  

---

## üìÅ Repository Structure

The project is structured to cleanly separate the evaluation logic, datasets, and solver implementations.

```text
AIMO_Project/
‚îÇ
‚îú‚îÄ‚îÄ README.md              # Project Documentation
‚îú‚îÄ‚îÄ requirements.txt       # Python Dependencies
‚îÇ
‚îú‚îÄ‚îÄ api/                   # Simulation of the Competition API
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ client.py          # The AIMOClient class
‚îÇ
‚îú‚îÄ‚îÄ evaluator/             # Internal Scoring Logic
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scoring.py         # Scoring rules and CSV generation
‚îÇ   ‚îî‚îÄ‚îÄ loader.py          # Problem dataset loader
‚îÇ
‚îú‚îÄ‚îÄ problems/              # Data Storage
‚îÇ   ‚îî‚îÄ‚îÄ sample_problems.json
‚îÇ
‚îú‚îÄ‚îÄ baselines/             # Example Solvers
‚îÇ   ‚îî‚îÄ‚îÄ simple_baseline.py
‚îÇ
‚îú‚îÄ‚îÄ utils/                 # (Optional) Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ helpers.py
‚îÇ
‚îú‚îÄ‚îÄ submission.csv         # Output file (Generated after running)
‚îî‚îÄ‚îÄ test_runner.py         # Main execution script
```

---

## üîç Component Details

### üîπ 1. API Layer (`api/`)

A simulated interface that mirrors how real competition servers interact with models.

| Function | Description |
|---------|-------------|
| `get_next()` | Returns the next unsolved problem from the queue |
| `submit(id, answer)` | Records the solver's prediction |
| `reset()` | Clears internal state for a fresh run |

---

### üîπ 2. Evaluator (`evaluator/`)

- **`loader.py`**  
  Loads JSON problems and ensures required fields:  
  - `id`  
  - `latex`  
  - `answer`

- **`scoring.py`**  
  - Compares model predictions with ground truth  
  - Computes a final score  
  - Generates `submission.csv`  
  - Prints a detailed scoring breakdown  

---

### üîπ 3. Problem Format (`problems/`)

All problems follow a strict schema inside `sample_problems.json`:

```json
{
  "id": "p4",
  "latex": "What is 10 + 10?",
  "answer": 20
}
```

---

## üõ† Installation & Setup

### ‚úÖ Prerequisites  
- Python **3.8+**

---

### ‚ñ∂Ô∏è Step 1: Verify Python Installation

```powershell
python --version
```

---

### ‚ñ∂Ô∏è Step 2: Navigate to the Project Directory

> Update the path according to your machine.

```powershell
cd C:\Users\Vittal\OneDrive\Desktop\Math_Olympiad\open-math-reasoning-suite\AIMO_Project
```

---

### ‚ñ∂Ô∏è Step 3: Install Dependencies

```powershell
pip install -r requirements.txt
```

---

## ‚ñ∂Ô∏è Usage Guide

### üî• Running the Evaluation

```powershell
python test_runner.py
```

### ‚úî Expected Output:

- A console log showing progress  
- Final score summary  
- A generated **`submission.csv`** in the project folder  

---

## ‚ûï Adding Custom Problems

Open:

```
problems/sample_problems.json
```

### Example Problem Entry:

```json
{
  "id": "p7",
  "latex": "Find the value of 3^3.",
  "answer": 27
}
```

Be sure to maintain **valid JSON formatting**.

---

## ü§ñ Adding Custom Solvers

Create your own solver in:

```
baselines/my_solver.py
```

Example template:

```python
from api.client import AIMOClient

def run():
    client = AIMOClient()
    problem = client.get_next()

    while problem:
        # Replace with your model's logic
        answer = 0
        client.submit(problem["id"], answer)
        problem = client.get_next()
```

Then modify `test_runner.py` to call your solver instead of the baseline.

---

## üìÑ License

This project is licensed under the **MIT License**.

---

## ‚≠ê Acknowledgements

Inspired by academic AI reasoning benchmarks including:

- AI Mathematical Olympiads  
- MATH dataset  
- GSM8K reasoning tasks  

---
