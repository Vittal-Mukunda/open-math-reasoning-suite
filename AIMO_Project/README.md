ğŸ§  AIMO Project â€“ Local Mathematical Olympiad Evaluation SystemA complete environment for loading, solving, submitting, and scoring AI-generated mathematical reasoning tasks.â­ OverviewThis repository simulates an AI Mathematical Olympiad (AIMO) competition environment. It provides a local evaluation sandbox identical in structure to official competitions, allowing you to:ğŸ“¥ Load Olympiad-style math problems.ğŸƒ Run a baseline or custom solver.ğŸ“ Submit answers to an internal evaluator.ğŸ“Š Score your modelâ€™s performance exactly like a real AI competition.ğŸ“„ Generate an automatic submission.csv.ğŸ“ Repository StructureThe project is organized to separate the evaluation logic from the problem datasets and solver implementations.PlaintextAIMO_Project/
â”‚
â”œâ”€â”€ README.md              # Project Documentation
â”œâ”€â”€ requirements.txt       # Python Dependencies
â”‚
â”œâ”€â”€ api/                   # Simulation of the Competition API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ client.py          # The AIMOClient class
â”‚
â”œâ”€â”€ evaluator/             # Internal Scoring Logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scoring.py         # Scoring rules and CSV generation
â”‚   â””â”€â”€ loader.py          # Problem dataset loader
â”‚
â”œâ”€â”€ problems/              # Data Storage
â”‚   â””â”€â”€ sample_problems.json
â”‚
â”œâ”€â”€ baselines/             # Example Solvers
â”‚   â””â”€â”€ simple_baseline.py
â”‚
â”œâ”€â”€ utils/                 # (Optional) Helper functions
â”‚   â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ submission.csv         # Output file (Generated after running)
â””â”€â”€ test_runner.py         # Main execution script
ğŸ” Component Details1. The API (api/)A simulated interface that mirrors how official AIMO competition servers interact with models.FunctionDescriptionget_next()Fetches the next problem object from the queue.submit(id, answer)Stores the prediction for the specific problem ID.reset()Clears the current state (useful for restarting runs).2. The Evaluator (evaluator/)loader.py: Loads problems from JSON, ensuring fields like id, latex, and answer exist.scoring.py: Compares predictions with ground truth, calculates the final score, and writes the results to submission.csv.3. Problem Format (problems/)Problems are stored in sample_problems.json following this strict schema:JSON{
  "id": "p4",
  "latex": "What is 10 + 10?",
  "answer": 20
}
ğŸ›  Installation & SetupPrerequisitesPython 3.8 or higherStep 1: Verify PythonPowerShellpython --version
Step 2: Clone & NavigateNavigate to the project directory (update the path to match your local machine).PowerShellcd C:\Users\Vittal\OneDrive\Desktop\Math_Olympiad\open-math-reasoning-suite\AIMO_Project
Step 3: Install DependenciesPowerShellpip install -r requirements.txt
â–¶ï¸ Usage GuideRunning the EvaluationTo run the full simulationâ€”which loads problems, runs the solver, and grades the resultsâ€”execute the test runner:PowerShellpython test_runner.py
Expected Output:Console log showing progress.Final Score summary.Generation of submission.csv in the root folder.Adding Custom ProblemsYou can extend the dataset by editing problems/sample_problems.json.Example:JSON[
  ...
  {
    "id": "p7",
    "latex": "Find the value of 3^3.",
    "answer": 27
  }
]
ğŸ¤– Adding Your Own SolverTo create a custom solver, add a new script in the baselines/ directory (e.g., my_solver.py).Template Code:Pythonfrom api.client import AIMOClient

def run():
    # Initialize the simulated API client
    client = AIMOClient()
    
    # Get the first problem
    problem = client.get_next()

    while problem is not None:
        print(f"Solving problem {problem['id']}...")
        
        # -----------------------------------------------
        # YOUR LOGIC GOES HERE
        # Example: answer = my_llm.generate(problem['latex'])
        # -----------------------------------------------
        
        # Placeholder logic
        answer = 0 

        # Submit the answer
        client.submit(problem["id"], answer)
        
        # Fetch next problem
        problem = client.get_next()
[!IMPORTANT]After creating your solver, remember to update test_runner.py to import and execute your new run() function instead of the baseline.ğŸ¤ ContributionContributions are welcome! If you'd like to improve the scoring logic or add new baseline models:Fork the repository.Create a feature branch (git checkout -b feature/NewSolver).Commit your changes.Push to the branch and open a Pull Request
