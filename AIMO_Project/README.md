# ğŸ§  AIMO Project â€“ Local Mathematical Olympiad Evaluation System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)

> **A complete environment for loading, solving, submitting, and scoring AI-generated mathematical reasoning tasks.**

---

## ğŸ“– Overview

This repository simulates an **AI Mathematical Olympiad (AIMO)** competition environment. It provides a local evaluation sandbox identical in structure to official competitions, allowing you to:

- ğŸ“¥ **Load** Olympiad-style math problems.
- ğŸƒ **Run** a baseline or custom solver.
- ğŸ“ **Submit** answers to an internal evaluator.
- ğŸ“Š **Score** your modelâ€™s performance exactly like a real AI competition.
- ğŸ“„ **Generate** an automatic `submission.csv`.

---

## ğŸ“ Repository Structure

The project is organized to separate the evaluation logic from the problem datasets and solver implementations.

```text
AIMO_Project/
â”‚
â”œâ”€â”€ README.md              # Project Documentation
â”œâ”€â”€ requirements.txt       # Python Dependencies
â”‚
â”œâ”€â”€ api/                   # Simulation of the Competition API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ client.py          # The AIMOClient class
â”‚
â”œâ”€â”€ evaluator/             # Internal Scoring Logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scoring.py         # Scoring rules and CSV generation
â”‚   â””â”€â”€ loader.py          # Problem dataset loader
â”‚
â”œâ”€â”€ problems/              # Data Storage
â”‚   â””â”€â”€ sample_problems.json
â”‚
â”œâ”€â”€ baselines/             # Example Solvers
â”‚   â””â”€â”€ simple_baseline.py
â”‚
â”œâ”€â”€ utils/                 # (Optional) Helper functions
â”‚   â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ submission.csv         # Output file (Generated after running)
â””â”€â”€ test_runner.py         # Main execution script

## ğŸ” Component Details

### 1. The API (`api/`)
A simulated interface that mirrors how official AIMO competition servers interact with models.

| Function | Description |
| :--- | :--- |
| `get_next()` | Fetches the next problem object from the queue. |
| `submit(id, answer)` | Stores the prediction for the specific problem ID. |
| `reset()` | Clears the current state (useful for restarting runs). |

### 2. The Evaluator (`evaluator/`)
- **`loader.py`**: Loads problems from JSON, ensuring fields like `id`, `latex`, and `answer` exist.
- **`scoring.py`**: Compares predictions with ground truth, calculates the final score, and writes the results to `submission.csv`.

### 3. Problem Format (`problems/`)
Problems are stored in `sample_problems.json` following this strict schema:

```json
{
  "id": "p4",
  "latex": "What is 10 + 10?",
  "answer": 20
}

## ğŸ›  Installation & Setup

### Prerequisites
* Python 3.8 or higher

### Step 1: Verify Python
```powershell
python --version

### Step 2: Clone & Navigate
Navigate to the project directory (update the path to match your local machine).

```powershell
cd C:\Users\Vittal\OneDrive\Desktop\Math_Olympiad\open-math-reasoning-suite\AIMO_Project

### Step 2: Dependencies
pip install -r requirements.txt

## â–¶ï¸ Usage Guide

### Running the Evaluation
To run the full simulationâ€”which loads problems, runs the solver, and grades the resultsâ€”execute the test runner:

```powershell
python test_runner.py

**Expected Output:**
1.  Console log showing progress.
2.  Final Score summary.
3.  Generation of `submission.csv` in the root folder.

### Adding Custom Problems
You can extend the dataset by editing `problems/sample_problems.json`.

**Example:**
```json
[
  ...
  {
    "id": "p7",
    "latex": "Find the value of 3^3.",
    "answer": 27
  }
]
